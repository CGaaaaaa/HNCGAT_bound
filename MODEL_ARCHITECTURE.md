# 增强版HNCGAT模型架构图

## 整体架构

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        增强版HNCGAT (Enhanced HNCGAT)                     │
└─────────────────────────────────────────────────────────────────────────┘

输入: 异质图 G = (V, E)
├── 节点类型: Protein (P), Metabolite (M), Functional Annotation (F/GO)
└── 边类型: PP, MM, PM, PF, MF

                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────────┐
│  阶段1: 图扩散模块 (Graph Diffusion Module)                              │
│  ──────────────────────────────────────────────────────────────────────  │
│                                                                           │
│  输入:                                                                   │
│    - adj_A_sim: [N_P, N_P] 蛋白相似度矩阵                                 │
│    - adj_B_sim: [N_M, N_M] 代谢物相似度矩阵                               │
│    - adj_AB: [N_P, N_M] 蛋白-代谢物交互矩阵                               │
│    - adj_AC: [N_P, N_F] 蛋白-GO关联矩阵                                   │
│    - adj_BC: [N_M, N_F] 代谢物-GO关联矩阵                                 │
│                                                                           │
│  步骤1: 构建Meta图（共邻居图）                                            │
│    A_meta = (PM·PM^T > 0) + (PF·PF^T > 0)  // P-M-P 和 P-F-P路径        │
│    B_meta = (MP^T·MP > 0) + (MF·MF^T > 0)  // M-P-M 和 M-F-M路径        │
│                                                                           │
│  步骤2: 融合相似图和Meta图                                                │
│    A_fused = adj_A_sim + β * A_meta  // β=0.5                            │
│    B_fused = adj_B_sim + β * B_meta  // β=0.5                            │
│                                                                           │
│  步骤3: 图扩散（多阶传播）                                                │
│    S = I + α·A_norm + α²·A_norm² + ... + α^K·A_norm^K                    │
│    其中:                                                                  │
│      - A_norm = D^{-1/2} · A_fused · D^{-1/2}  // 对称归一化            │
│      - K = 3 (扩散步数)                                                   │
│      - α = 0.2 (扩散强度)                                                 │
│                                                                           │
│  输出:                                                                   │
│    - diff_A_sim: [N_P, N_P] 扩散后的蛋白相似度                            │
│    - diff_B_sim: [N_M, N_M] 扩散后的代谢物相似度                          │
│                                                                           │
└─────────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────────┐
│  阶段2: 异质图注意力编码器 (Heterogeneous Graph Attention Encoder)        │
│  ──────────────────────────────────────────────────────────────────────  │
│                                                                           │
│  输入:                                                                   │
│    - nodeA_feature: [N_P, d] 蛋白初始特征                                 │
│    - nodeB_feature: [N_M, d] 代谢物初始特征                               │
│    - nodeC_feature: [N_F, d] GO初始特征                                   │
│    - diff_A_sim: 扩散后的蛋白相似图                                        │
│    - diff_B_sim: 扩散后的代谢物相似图                                      │
│                                                                           │
│  蛋白质节点嵌入:                                                          │
│    e_P = ReLU(W_P · x_P)                                                 │
│    e_PP = ReLU(W_PP · diff_A_sim[i,:])  // 从扩散图聚合                  │
│    e_PM = Attention(e_P, x_M, adj_AB)    // 异质注意力聚合代谢物邻居      │
│    e_PF = Attention(e_P, x_F, adj_AC)    // 异质注意力聚合GO邻居         │
│    h_P = Concat([e_P, e_PP, e_PM, e_PF]) // [N_P, 4d]                    │
│                                                                           │
│  代谢物节点嵌入:                                                          │
│    e_M = ReLU(W_M · x_M)                                                 │
│    e_MM = ReLU(W_MM · diff_B_sim[i,:])  // 从扩散图聚合                  │
│    e_MP = Attention(e_M, x_P, adj_AB^T)  // 异质注意力聚合蛋白邻居        │
│    e_MF = Attention(e_M, x_F, adj_BC)    // 异质注意力聚合GO邻居         │
│    h_M = Concat([e_M, e_MM, e_MP, e_MF]) // [N_M, 4d]                    │
│                                                                           │
│  输出:                                                                   │
│    - embP: [N_P, hidden_dim] 蛋白节点嵌入                                 │
│    - embM: [N_M, hidden_dim] 代谢物节点嵌入                               │
│    - embF: [N_F, hidden_dim] GO节点嵌入                                   │
│                                                                           │
└─────────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────────┐
│  阶段3: 加权邻居对比学习 (Weighted Neighbor Contrastive Learning)         │
│  ──────────────────────────────────────────────────────────────────────  │
│                                                                           │
│  为蛋白质节点计算对比损失:                                                 │
│                                                                           │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ 创新点1: 自适应权重学习                                            │   │
│  │                                                                 │   │
│  │  输入: embP, embM, embF, adj_PM, adj_PP, adj_PF                 │   │
│  │                                                                 │   │
│  │  步骤1: 聚合邻居表示                                              │   │
│  │    PM_repr = Aggregate(embM, adj_PM)  // 代谢物邻居              │   │
│  │    PP_repr = Aggregate(embP, adj_PP)  // 蛋白邻居                │   │
│  │    PF_repr = Aggregate(embF, adj_PF)  // GO邻居                  │   │
│  │                                                                 │   │
│  │  步骤2: 学习节点特定权重                                           │   │
│  │    weights = Attention_MLP(PM_repr, PP_repr, PF_repr)           │   │
│  │    weights = Softmax(weights / T)  // T=0.8 (温度参数)           │   │
│  │    输出: [w_PM, w_PP, w_PF]  // 每个节点有自己的一组权重          │   │
│  │                                                                 │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                           │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ 创新点2: 难负样本挖掘 (Hard Negative Mining)                      │   │
│  │                                                                 │   │
│  │  对于每个anchor节点:                                             │   │
│  │    1. 计算与所有负样本的相似度                                     │   │
│  │    2. 选择top-K个最相似的负样本（hard negatives）                 │   │
│  │    3. 给hard negatives更大的权重（weight=2.0）                    │   │
│  │                                                                 │   │
│  │  示例（蛋白质节点v_P^i）:                                        │   │
│  │    Positive pairs:                                              │   │
│  │      - (v_P^i, v_M^j) if adj_PM[i,j] > 0                        │   │
│  │      - (v_P^i, v_P^j) if adj_PP[i,j] > 0                        │   │
│  │      - (v_P^i, v_F^j) if adj_PF[i,j] > 0                        │   │
│  │                                                                 │   │
│  │    Negative pairs:                                              │   │
│  │      - 所有 (v_P^i, v_P^j) where adj_PP[i,j] == 0               │   │
│  │      - 选择top-30%最相似的作为hard negatives                     │   │
│  │                                                                 │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                           │
│  加权对比损失:                                                            │
│    L_contrast = -log(                                                    │
│        Σ(w_PM·sim_pos_PM + w_PP·sim_pos_PP + w_PF·sim_pos_PF) /        │
│        (Σ(w_PM·sim_pos_PM) + Σ(w_PP·sim_all_PP) + Σ(w_PF·sim_all_PF))  │
│    )                                                                     │
│    其中 sim_all 包括正样本和hard negatives                                │
│                                                                           │
│  同样计算M和F的对比损失                                                   │
│    L_total = (L_P + L_M + L_F) / 3                                       │
│                                                                           │
└─────────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────────┐
│  阶段4: 边预测解码器 (Edge Prediction Decoder)                           │
│  ──────────────────────────────────────────────────────────────────────  │
│                                                                           │
│  输入:                                                                   │
│    - 节点对: (protein_i, metabolite_j)                                   │
│    - embP[i]: 蛋白i的嵌入                                                │
│    - embM[j]: 代谢物j的嵌入                                              │
│                                                                           │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ 创新点3: 边特征增强 (Edge Feature Augmentation)                   │   │
│  │                                                                 │   │
│  │  提取边特征（7维）:                                              │   │
│  │    1. common_GO: 共同GO term数量                                 │   │
│  │       = Σ(adj_AC[i,:] · adj_BC[j,:])                            │   │
│  │                                                                 │   │
│  │    2. protein_degree: 蛋白i的代谢物邻居数量                       │   │
│  │       = Σ(adj_AB[i,:])                                          │   │
│  │                                                                 │   │
│  │    3. metabolite_degree: 代谢物j的蛋白邻居数量                    │   │
│  │       = Σ(adj_AB[:,j])                                          │   │
│  │                                                                 │   │
│  │    4. protein_avg_sim: 蛋白i在PP图上的平均相似度                  │   │
│  │       = mean(diff_A_sim[i,:])                                   │   │
│  │                                                                 │   │
│  │    5. metabolite_avg_sim: 代谢物j在MM图上的平均相似度             │   │
│  │       = mean(diff_B_sim[j,:])                                   │   │
│  │                                                                 │   │
│  │    6. protein_diff_sim: 蛋白i在扩散图上的平均相似度               │   │
│  │       = mean(diff_A_sim[i,:])  // 如果使用扩散                    │   │
│  │                                                                 │   │
│  │    7. metabolite_diff_sim: 代谢物j在扩散图上的平均相似度          │   │
│  │       = mean(diff_B_sim[j,:])  // 如果使用扩散                    │   │
│  │                                                                 │   │
│  │  内存优化: 分批处理（每批2000个样本）                               │   │
│  │                                                                 │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                           │
│  构建边表示:                                                              │
│    if edgetype == 'concat':                                              │
│        edge_emb = Concat([embP[i], embM[j], edge_features])             │
│        // [hidden_dim*2 + 7]                                            │
│    elif edgetype == 'L1':                                                │
│        edge_emb = Concat([|embP[i] - embM[j]|, edge_features])          │
│    // ... 其他操作符                                                      │
│                                                                           │
│  MLP分类器:                                                               │
│    h1 = ReLU(Dropout(Linear(edge_emb → hidden_dim)))                    │
│    h2 = ReLU(Dropout(Linear(hidden_dim → hidden_dim/2)))                │
│    prob = Sigmoid(Linear(hidden_dim/2 → 1))                             │
│                                                                           │
│  输出: 预测概率 p(protein_i, metabolite_j) ∈ [0, 1]                      │
│                                                                           │
└─────────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────────┐
│  损失函数 (Loss Function)                                                 │
│  ──────────────────────────────────────────────────────────────────────  │
│                                                                           │
│  L_total = L_BCE + λ · L_contrast                                        │
│                                                                           │
│  其中:                                                                    │
│    - L_BCE: 加权二元交叉熵损失（正样本权重=10，负样本权重=1）              │
│    - L_contrast: 加权邻居对比损失（包含hard negative mining）             │
│    - λ = 1.0 (平衡参数)                                                  │
│                                                                           │
└─────────────────────────────────────────────────────────────────────────┘
```

## 详细模块说明

### 1. 图扩散模块（Graph Diffusion Module）

```
输入: 原始相似度图
│
├─> 步骤1: 构建Meta图
│   │
│   ├─> A_meta = (PM × PM^T) OR (PF × PF^T)
│   │   // 两个蛋白通过代谢物或GO共邻居连接
│   │
│   └─> B_meta = (MP^T × MP) OR (MF × MF^T)
│       // 两个代谢物通过蛋白或GO共邻居连接
│
├─> 步骤2: 融合
│   │
│   ├─> A_fused = A_sim + β × A_meta  (β = 0.5)
│   └─> B_fused = B_sim + β × B_meta  (β = 0.5)
│
└─> 步骤3: 多阶扩散
    │
    ├─> 归一化: A_norm = D^{-1/2} × A_fused × D^{-1/2}
    │
    ├─> 扩散: S = I + α·A_norm + α²·A_norm² + α³·A_norm³
    │   (K=3, α=0.2)
    │
    └─> 输出: diff_A_sim, diff_B_sim
```

**可视化**:
```
原始相似图          Meta图（共邻居）      融合后            扩散后
  A -- B            A -- C -- B         A -- B (strong)   A ~~~ B (更强)
  |    |            |    |    |         |    |            |     |
  C    D            E    F    D         C ~~ D            C ~~~ D
```

### 2. 加权邻居对比学习（Weighted Neighbor Contrastive Learning）

```
对于每个蛋白质节点 v_P^i:

┌──────────────────────────────────────────────────────────────┐
│ 正样本对 (Positive Pairs):                                    │
│                                                               │
│  v_P^i  ←─PM─→  v_M^j  (权重: w_PM = 0.27)                   │
│         ←─PP─→  v_P^k  (权重: w_PP = 0.18)                   │
│         ←─PF─→  v_F^l  (权重: w_PF = 0.55)                   │
│                                                               │
│  权重是节点特定的，通过Attention MLP学习                        │
└──────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────┐
│ 负样本对 (Negative Pairs):                                    │
│                                                               │
│  v_P^i  ────X──→  v_P^m  (普通负样本，权重=1.0)                │
│         ────X──→  v_P^n  (普通负样本，权重=1.0)                │
│         ────X──→  v_P^o  (Hard Negative，权重=2.0) ⭐         │
│         ────X──→  v_P^p  (Hard Negative，权重=2.0) ⭐         │
│                                                               │
│  Hard Negatives是相似度最高的top-30%负样本                      │
└──────────────────────────────────────────────────────────────┘
```

### 3. 边特征增强（Edge Feature Augmentation）

```
边 (protein_i, metabolite_j) 的特征提取:

Node Embeddings          Edge Features (7维)
┌──────────┐            ┌──────────────────────────────┐
│ embP[i]  │            │ 1. common_GO = 5             │
│          │   ────>    │ 2. protein_degree = 12       │
│ [64-dim] │   Concatenate │ 3. metabolite_degree = 8  │
│          │            │ 4. protein_avg_sim = 0.42    │
└──────────┘            │ 5. metabolite_avg_sim = 0.38 │
┌──────────┐            │ 6. protein_diff_sim = 0.45   │
│ embM[j]  │            │ 7. metabolite_diff_sim=0.41  │
│          │            └──────────────────────────────┘
│ [64-dim] │                    │
└──────────┘                    ▼
                        [128 + 7 = 135-dim]
                                │
                                ▼
                         MLP分类器
                                │
                                ▼
                           预测概率
```

### 4. 内存优化策略

```
大batch (200000个样本对)
    │
    ▼
分批处理 (每批2000个)
    │
    ├─> Batch 1: indices[0:2000]
    │   ├─> 提取边特征
    │   ├─> 清理内存 (del + torch.cuda.empty_cache())
    │   └─> 保存特征
    │
    ├─> Batch 2: indices[2000:4000]
    │   └─> ...
    │
    └─> Batch N: indices[N-2000:N]
        └─> ...
    │
    ▼
拼接所有批次特征
    │
    ▼
送入MLP解码器
```

## 数据流图

```
输入数据
    │
    ├─> Proteins (12641) ──┐
    ├─> Metabolites (1331) ─┤
    └─> GO terms (7681) ────┘
            │
            ▼
    ┌───────────────┐
    │ 图扩散模块     │ ──> diff_A_sim [12641×12641]
    │ (可选)        │ ──> diff_B_sim [1331×1331]
    └───────────────┘
            │
            ▼
    ┌───────────────┐
    │ 异质GAT编码器  │ ──> embP [12641×64]
    │               │ ──> embM [1331×64]
    │               │ ──> embF [7681×64]
    └───────────────┘
            │
            ├──────────────────────────────────┐
            │                                  │
            ▼                                  ▼
    ┌───────────────┐              ┌──────────────────────┐
    │ 加权对比损失   │              │ 边特征增强解码器      │
    │ (训练时)      │              │                      │
    │               │              │ 边特征 [batch×7]     │
    │ L_contrast    │              │ + Node emb [batch×128]│
    └───────────────┘              └──────────────────────┘
            │                                  │
            └──────────────┬───────────────────┘
                           │
                           ▼
                    ┌──────────────┐
                    │ 总损失        │
                    │ L_total =    │
                    │ L_BCE + λ·L_c│
                    └──────────────┘
                           │
                           ▼
                        反向传播
```

## 关键创新点总结

### 创新1: 图扩散（已有）
- **位置**: 编码器输入阶段
- **作用**: 补全高阶关系，提升全局结构建模
- **参数**: K=3, α=0.2, β=0.5

### 创新2: 加权邻居对比损失（已有）
- **位置**: 对比学习阶段
- **作用**: 为不同邻居类型学习自适应权重
- **参数**: T=0.8 (温度参数)

### 创新3: 难负样本挖掘（新增）
- **位置**: 对比学习阶段
- **作用**: 只关注最难的负样本，学习更精准的决策边界
- **参数**: ratio=0.3, weight=2.0

### 创新4: 边特征增强（新增）
- **位置**: 解码器阶段
- **作用**: 在node embedding基础上加入结构特征
- **参数**: 7维特征，分批处理（batch_size=2000）

## 模块依赖关系

```
图扩散模块
    ↓ (输出扩散图)
异质GAT编码器 ←─── 原始相似图
    ↓ (输出节点嵌入)
    ├─> 加权对比损失模块 ←─── 难负样本挖掘
    │
    └─> 边特征解码器 ←─── 边特征提取 ←─── 扩散图
            ↓
        预测概率
```

## 实验结果（当前）

### 基线（30%训练，Weighted + Diffusion）
- AUC: **0.9224±0.0026**
- AP: **0.6717±0.0619**

### 原始HNCGAT（30%训练）
- AUC: **0.933±0.002**
- AP: **0.730±0.017**

**分析**: 基线结果略低于原始HNCGAT，说明扩散在小样本下可能引入噪声。

### 预期（加入新创新后）
- **Hard Negative**: AUC +0.5~1%, AP +1~2%
- **Edge Features**: AUC +0.5~1%, AP +1~2%
- **Both**: AUC +1~2%, AP +2~3%

---

## 内存使用分析

| 模块 | 内存峰值 | 说明 |
|------|----------|------|
| 图扩散 | ~1-2 GB | 扩散矩阵计算 |
| 异质GAT编码 | ~2-3 GB | 注意力机制 |
| 加权对比损失 | ~1-2 GB | 相似度计算 |
| **边特征提取** | **~6-8 GB** | **大batch索引操作** ⚠️ |
| MLP解码器 | ~1 GB | 分类器 |

**总内存**: 约10-15 GB（取决于batch大小）

**优化策略**: 
- 边特征提取分批处理（每批2000个样本）
- 每批后清理中间变量
- 使用torch.cuda.empty_cache()释放GPU缓存

